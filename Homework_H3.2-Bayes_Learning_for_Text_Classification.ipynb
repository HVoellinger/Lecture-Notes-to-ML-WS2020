{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Text Classification\n",
    "\n",
    "We made a simple Algorithm to try and classify senteces into either Sports or Not Sports sentences.\n",
    "We start with a couple sentences either classed \"Sports\" or \"Not Sports\" and try to classify new sentences based on that.\n",
    "At the end we make a comparison, which class (\"Sports\" or \"Not Sports\") the new sentence is more likely to end up in.\n",
    "\n",
    "## What happens here:\n",
    "\n",
    "    1. import everything we need\n",
    "    2. Provide training data and do transformations.\n",
    "    3. Create dictionaries and count the words in each class.\n",
    "    4. Calculate probabilities of the words.\n",
    "    \n",
    "To evaluate a new sentence...\n",
    "\n",
    "    5. Vectorize and transform all sentences\n",
    "    6. Count all words\n",
    "    7. Transform new sentence\n",
    "    8. Perform Laplace Smoothing, so we dont multiply with 0\n",
    "    9. Calculate probability of the new sentence for each class\n",
    "    10. Output whats more likely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.5.zip (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 3.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting click\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "\u001b[K     |████████████████████████████████| 82 kB 670 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib in /srv/conda/envs/notebook/lib/python3.6/site-packages (from nltk) (0.16.0)\n",
      "Collecting regex\n",
      "  Downloading regex-2020.10.11-cp36-cp36m-manylinux2010_x86_64.whl (662 kB)\n",
      "\u001b[K     |████████████████████████████████| 662 kB 17.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.50.2-py2.py3-none-any.whl (70 kB)\n",
      "\u001b[K     |████████████████████████████████| 70 kB 7.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434676 sha256=e46e5059d19cafa1200fcfa66ce028ea32bfdcbc69b12cb6dad1180086ac2e0d\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/de/5e/42/64abaeca668161c3e2cecc24f864a8fc421e3d07a104fc8a51\n",
      "Successfully built nltk\n",
      "Installing collected packages: click, regex, tqdm, nltk\n",
      "Successfully installed click-7.1.2 nltk-3.5 regex-2020.10.11 tqdm-4.50.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This notebook was created by Alireza Gholami and Jannik Schwarz\n",
    "\n",
    "\n",
    "# Importing everything we need\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training data:\n",
      "                       sentence       class\n",
      "0                  A great game      Sports\n",
      "1         The election was over  Not Sports\n",
      "2              Very clean match      Sports\n",
      "3  A clean but forgettable game      Sports\n",
      "4       It was a close election  Not Sports\n",
      "5             A very close game      Sports\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Naming the columns\n",
    "columns = ['sentence', 'class']\n",
    "\n",
    "# Our training data\n",
    "rows = [['A great game', 'Sports'],\n",
    "        ['The election was over', 'Not Sports'],\n",
    "        ['Very clean match', 'Sports'],\n",
    "        ['A clean but forgettable game', 'Sports'],\n",
    "        ['It was a close election', 'Not Sports'],\n",
    "        ['A very close game', 'Sports']]\n",
    "\n",
    "# the data inside a dataframe\n",
    "training_data = pd.DataFrame(rows, columns=columns)\n",
    "print(f'The training data:\\n{training_data}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turns the data into vectors\n",
    "def vectorisation(my_class):\n",
    "    \n",
    "    # my_docs contains the sentences for a class (sports or not sports)\n",
    "    my_docs = [row['sentence'] for index, row in training_data.iterrows() if row['class'] == my_class]\n",
    "    \n",
    "    # creates a vector that counts the occurence of words in a sentence\n",
    "    my_vector = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\") # Token-Pattern damit einstellige Wörter wie 'a' gelesen werden\n",
    "    \n",
    "    # transform the sentences\n",
    "    my_x = my_vector.fit_transform(my_docs)\n",
    "    \n",
    "    # tdm = term_document_matrix_sport | create the matrix with the vectors for a class\n",
    "    tdm = pd.DataFrame(my_x.toarray(), columns=my_vector.get_feature_names())\n",
    "    return tdm, my_vector, my_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sport sentence matrix: \n",
      "   a  but  clean  close  forgettable  game  great  match  very\n",
      "0  1    0      0      0            0     1      1      0     0\n",
      "1  0    0      1      0            0     0      0      1     1\n",
      "2  1    1      1      0            1     1      0      0     0\n",
      "3  1    0      0      1            0     1      0      0     1\n",
      "\n",
      "Not sport sentence matrix: \n",
      "   a  close  election  it  over  the  was\n",
      "0  0      0         1   0     1    1    1\n",
      "1  1      1         1   1     0    0    1\n",
      "\n",
      "Amount of sport sentences: 4\n",
      "Amount of not sport senteces: 2\n",
      "Total amount of sentences: 6\n"
     ]
    }
   ],
   "source": [
    "# Here we are actually creating the matrix for sport and not sport sentences\n",
    "tdm_sport, vector_sport, X_sport = vectorisation('Sports')\n",
    "tdm_not_sport, vector_not_sport, X_not_sport = vectorisation('Not Sports')\n",
    "\n",
    "print(f'Sport sentence matrix: \\n{tdm_sport}\\n')\n",
    "print(f'Not sport sentence matrix: \\n{tdm_not_sport}\\n')\n",
    "print(f'Amount of sport sentences: {len(tdm_sport)}')\n",
    "print(f'Amount of not sport senteces: {len(tdm_not_sport)}')\n",
    "print(f'Total amount of sentences: {len(rows)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a dictionary for each class\n",
    "def make_list(my_vector, my_x):\n",
    "    my_word_list = my_vector.get_feature_names()\n",
    "    my_count_list = my_x.toarray().sum(axis=0)\n",
    "    my_freq = dict(zip(my_word_list, my_count_list))\n",
    "    return my_word_list, my_count_list, my_freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sport dictionary: \n",
      "{'a': 3, 'but': 1, 'clean': 2, 'close': 1, 'forgettable': 1, 'game': 3, 'great': 1, 'match': 1, 'very': 2}\n",
      "\n",
      "not sport dictionary: \n",
      "{'a': 1, 'close': 1, 'election': 2, 'it': 1, 'over': 1, 'the': 1, 'was': 2}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create lists\n",
    "\n",
    "# word_list_sport = word list ['a', 'but', 'clean', 'forgettable', 'game', 'great', 'match', 'very']\n",
    "# count_list_sport = occurence of words [2 1 2 1 2 1 1 1]\n",
    "# freq_sport = combining the two to create a dictionary\n",
    "word_list_sport, count_list_sport, freq_sport = make_list(vector_sport, X_sport)\n",
    "word_list_not_sport, count_list_not_sport, freq_not_sport = make_list(vector_not_sport, X_not_sport)\n",
    "\n",
    "print(f'sport dictionary: \\n{freq_sport}\\n')\n",
    "print(f'not sport dictionary: \\n{freq_not_sport}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the probabilty of a word in a sentence of a class\n",
    "def calculate_prob(my_word_list, my_count_list):\n",
    "    my_prob = []\n",
    "    for my_word, my_count in zip(my_word_list, my_count_list):\n",
    "        my_prob.append(my_count / len(my_word_list))\n",
    "    prob_dict = dict(zip(my_word_list, my_prob))\n",
    "    return prob_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabilites of words in sport sentences: \n",
      "{'a': 0.3333333333333333, 'but': 0.1111111111111111, 'clean': 0.2222222222222222, 'close': 0.1111111111111111, 'forgettable': 0.1111111111111111, 'game': 0.3333333333333333, 'great': 0.1111111111111111, 'match': 0.1111111111111111, 'very': 0.2222222222222222}\n",
      "\n",
      "probabilites of words in not sport sentences: \n",
      "{'a': 0.14285714285714285, 'close': 0.14285714285714285, 'election': 0.2857142857142857, 'it': 0.14285714285714285, 'over': 0.14285714285714285, 'the': 0.14285714285714285, 'was': 0.2857142857142857}\n"
     ]
    }
   ],
   "source": [
    "# probabilities of the words in a class\n",
    "prob_sport_dict = calculate_prob(word_list_sport, count_list_sport)\n",
    "prob_not_sport_dict = calculate_prob(word_list_not_sport, count_list_not_sport)\n",
    "print(f'probabilites of words in sport sentences: \\n{prob_sport_dict}\\n')\n",
    "print(f'probabilites of words in not sport sentences: \\n{prob_not_sport_dict}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of distinct words: 14\n",
      "Amount of distinct words in sport sentences: 15\n",
      "Amount of distinct words in not sport sentences: 9\n"
     ]
    }
   ],
   "source": [
    "# all sentences again\n",
    "docs = [row['sentence'] for index, row in training_data.iterrows()]\n",
    "\n",
    "# vectorizer\n",
    "vector = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "\n",
    "# transform the sentences\n",
    "X = vector.fit_transform(docs)\n",
    "\n",
    "# counting the words\n",
    "total_features = len(vector.get_feature_names())\n",
    "total_counts_features_sport = count_list_sport.sum(axis=0)\n",
    "total_counts_features_not_sport = count_list_not_sport.sum(axis=0)\n",
    "                     \n",
    "print(f'Amount of distinct words: {total_features}')\n",
    "print(f'Amount of distinct words in sport sentences: {total_counts_features_sport}')\n",
    "print(f'Amount of distinct words in not sport sentences: {total_counts_features_not_sport}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a new sentence \n",
    "new_sentence = 'Hermann plays a TT match'\n",
    "\n",
    "# gets tokenized\n",
    "new_word_list = word_tokenize(new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're using laplace smoothing\n",
    "# if a new word occurs the probability would be 0\n",
    "# So every word counter gets incremented by one\n",
    "def laplace(freq, total_count, total_feat):\n",
    "    prob_sport_or_not = []\n",
    "    for my_word in new_word_list:\n",
    "        if my_word in freq.keys():\n",
    "            counter = freq[my_word]\n",
    "        else:\n",
    "            counter = 0\n",
    "        # total_count is the amount of words in sport sentences and total_feat the total amount of words\n",
    "        prob_sport_or_not.append((counter + 1) / (total_count + total_feat))\n",
    "    return prob_sport_or_not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability that the word is in a sport sentece: [0.034482758620689655, 0.034482758620689655, 0.13793103448275862, 0.034482758620689655, 0.06896551724137931]\n",
      "probability that the word is in a not sport sentece: [0.043478260869565216, 0.043478260869565216, 0.08695652173913043, 0.043478260869565216, 0.043478260869565216]\n"
     ]
    }
   ],
   "source": [
    "# probability for the new words\n",
    "prob_new_sport = laplace(freq_sport, total_counts_features_sport, total_features)\n",
    "prob_new_not_sport = laplace(freq_not_sport, total_counts_features_not_sport, total_features)\n",
    "\n",
    "print(f'probability that the word is in a sport sentece: {prob_new_sport}')\n",
    "print(f'probability that the word is in a not sport sentece: {prob_new_not_sport}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiplying the probabilities of each word\n",
    "new_sport = list(prob_new_sport)\n",
    "sport_multiply_result = 1\n",
    "for i in range(0, len(new_sport)):\n",
    "    sport_multiply_result *= new_sport[i]\n",
    "\n",
    "# multiplying the result with the ratio of sports senteces to the total amount of sentences (here its 4/6)\n",
    "sport_multiply_result *= ( len(tdm_sport) / len(rows) )\n",
    "\n",
    "# multiplying the probabilities of each word   \n",
    "new_not_sport = list(prob_new_not_sport)\n",
    "not_sport_multiply_result = 1\n",
    "for i in range(0, len(new_not_sport)):\n",
    "    not_sport_multiply_result *= new_not_sport[i]\n",
    "    \n",
    "# multiplying the result with the ratio of sports senteces to the total amount of sentences (here its 2/6)\n",
    "not_sport_multiply_result *= ( len(tdm_not_sport) / len(rows) )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of the sentence \"Hermann plays a TT match\":\n",
      "Sport vs not sport\n",
      "2.6002118815154297e-07 vs 1.0357848652047699e-07\n",
      "\n",
      "\n",
      "Verdict: It's probably a sports sentence!\n"
     ]
    }
   ],
   "source": [
    "# comparing whats more likely \n",
    "\n",
    "print(f'The probability of the sentence \"{new_sentence}\":\\nSport vs not sport\\n{sport_multiply_result} vs {not_sport_multiply_result}\\n\\n')\n",
    "\n",
    "if not_sport_multiply_result < sport_multiply_result:\n",
    "    print('Verdict: It\\'s probably a sports sentence!')\n",
    "else:\n",
    "    print('Verdict: It\\'s probably not a sport sentence!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
